{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5cce87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run only once, then comment out)\n",
    "# !pip install pandas matplotlib seaborn numpy scikit-learn joblib xgboost parameterized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74061d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from abc import ABC, abstractmethod\n",
    "import os\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import joblib\n",
    "import tempfile\n",
    "import shutil\n",
    "import unittest\n",
    "from parameterized import parameterized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2778ced1",
   "metadata": {},
   "source": [
    "METRICS BASE CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41838547",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metric(ABC):\n",
    "    \"\"\"Abstract base class for evaluation metrics. => define base common methods\"\"\"\n",
    "    @abstractmethod\n",
    "    def calculate(self, y_true, y_pred, **kwargs):\n",
    "        \"\"\"Calculate the metric value.\n",
    "        \n",
    "        Args:\n",
    "            y_true: Array-like of true values.\n",
    "            y_pred: Array-like of predicted values.\n",
    "            **kwargs: Optional metric-specific parameters.\n",
    "        \n",
    "        Returns:\n",
    "            Float representing the metric value.\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: If inputs are invalid (e.g., mismatched lengths, non-numeric).\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def name(self):\n",
    "        \"\"\"Return the metric's name.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def description(self):\n",
    "        \"\"\"Return a description of the metric.\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efb922c",
   "metadata": {},
   "source": [
    "CONCRETE METRIC CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b54271a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class R2Metric(Metric):\n",
    "    \"\"\"RÂ² (Coefficient of Determination) metric.\"\"\"\n",
    "    def calculate(self, y_true, y_pred, **kwargs):\n",
    "        y_true, y_pred = self._validate_inputs(y_true, y_pred)\n",
    "        return r2_score(y_true, y_pred)\n",
    "    \n",
    "    def name(self):\n",
    "        return \"R2\"\n",
    "    \n",
    "    def description(self):\n",
    "        return \"Coefficient of Determination, measuring variance explained by the model.\"\n",
    "    \n",
    "    def _validate_inputs(self, y_true, y_pred):\n",
    "        \"\"\"Validate inputs for metric calculation.\"\"\"\n",
    "        y_true = np.array(y_true, dtype=float)\n",
    "        y_pred = np.array(y_pred, dtype=float)\n",
    "        if len(y_true) != len(y_pred):\n",
    "            raise ValueError(\"y_true and y_pred must have the same length\")\n",
    "        if len(y_true) == 0:\n",
    "            raise ValueError(\"Input arrays cannot be empty\")\n",
    "        if not np.all(np.isfinite(y_true)) or not np.all(np.isfinite(y_pred)):\n",
    "            raise ValueError(\"Inputs must be numeric and finite\")\n",
    "        return y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e065809d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAEMetric(Metric):\n",
    "    \"\"\"Mean Absolute Error (MAE) metric.\"\"\"\n",
    "    def calculate(self, y_true, y_pred, **kwargs):\n",
    "        y_true, y_pred = self._validate_inputs(y_true, y_pred)\n",
    "        return mean_absolute_error(y_true, y_pred)\n",
    "    \n",
    "    def name(self):\n",
    "        return \"MAE\"\n",
    "    \n",
    "    def description(self):\n",
    "        return \"Mean Absolute Error, measuring average absolute difference between predictions and actuals.\"\n",
    "    \n",
    "    def _validate_inputs(self, y_true, y_pred):\n",
    "        return R2Metric._validate_inputs(self, y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c976e415",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSEMetric(Metric):\n",
    "    \"\"\"Root Mean Squared Error (RMSE) metric.\"\"\"\n",
    "    def calculate(self, y_true, y_pred, **kwargs):\n",
    "        y_true, y_pred = self._validate_inputs(y_true, y_pred)\n",
    "        return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    \n",
    "    def name(self):\n",
    "        return \"RMSE\"\n",
    "    \n",
    "    def description(self):\n",
    "        return \"Root Mean Squared Error, measuring square root of average squared differences.\"\n",
    "    \n",
    "    def _validate_inputs(self, y_true, y_pred):\n",
    "        return R2Metric._validate_inputs(self, y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557453a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New test metric\n",
    "class MAPEMetric(Metric):\n",
    "    \"\"\"Symmetric Mean Absolute Percentage Error (sMAPE) metric.\"\"\"\n",
    "    def calculate(self, y_true, y_pred, **kwargs):\n",
    "        y_true, y_pred = self._validate_inputs(y_true, y_pred)\n",
    "        # Compute absolute differences and denominators\n",
    "        absolute_diff = np.abs(y_true - y_pred)\n",
    "        denominator = np.abs(y_true) + np.abs(y_pred)\n",
    "        # Handle zero denominators with a small epsilon\n",
    "        epsilon = 1e-10\n",
    "        mask = denominator == 0\n",
    "        denominator[mask] = epsilon\n",
    "        # Compute percentage errors and average them\n",
    "        percentage_errors = (absolute_diff / denominator) * 100\n",
    "        return np.mean(percentage_errors)\n",
    "    \n",
    "    def name(self):\n",
    "        return \"sMAPE\"\n",
    "    \n",
    "    def description(self):\n",
    "        return \"Symmetric Mean Absolute Percentage Error, measuring average percentage difference with symmetry, robust to zero values.\"\n",
    "    \n",
    "    def _validate_inputs(self, y_true, y_pred):\n",
    "        return R2Metric._validate_inputs(self, y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bfb264",
   "metadata": {},
   "source": [
    "EVALUATOR - MAIN IMPLEMENTATION/METRIC-CALLING CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcd9b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    \"\"\"Class to evaluate predictions using multiple metrics.\"\"\"\n",
    "    def __init__(self, metrics):\n",
    "        \"\"\"Initialize with a list of Metric objects.\n",
    "        \n",
    "        Args:\n",
    "            metrics: List of Metric objects to evaluate.\n",
    "        \"\"\"\n",
    "        self.metrics = metrics\n",
    "    \n",
    "    def evaluate_from_csv(self, csv_path, true_col=\"Actual\", pred_col=\"Predicted\"):\n",
    "        \"\"\"Evaluate predictions from a CSV file.\n",
    "        \n",
    "        Args:\n",
    "            csv_path: Path to CSV file with true and predicted values.\n",
    "            true_col: Name of column with true values (default: 'Actual').\n",
    "            pred_col: Name of column with predicted values (default: 'Predicted').\n",
    "        \n",
    "        Returns:\n",
    "            Dict with metric names as keys and values as floats.\n",
    "        \n",
    "        Raises:\n",
    "            FileNotFoundError: If CSV file does not exist.\n",
    "            ValueError: If columns are missing or data is invalid.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(csv_path):\n",
    "            raise FileNotFoundError(f\"CSV file not found: {csv_path}\")\n",
    "        \n",
    "        df = pd.read_csv(csv_path)\n",
    "        if true_col not in df.columns or pred_col not in df.columns:\n",
    "            raise ValueError(f\"Columns {true_col} and/or {pred_col} not found in CSV\")\n",
    "        \n",
    "        y_true = df[true_col]\n",
    "        y_pred = df[pred_col]\n",
    "        \n",
    "        results = {}\n",
    "        for metric in self.metrics:\n",
    "            try:\n",
    "                results[metric.name()] = metric.calculate(y_true, y_pred)\n",
    "            except ValueError as e:\n",
    "                print(f\"Error calculating {metric.name()}: {e}\")\n",
    "        return results\n",
    "\n",
    "    def evaluate_models(self, models, x_data, y_true, model_names):\n",
    "        \"\"\"Evaluate multiple models on provided data.\n",
    "        \n",
    "        Args:\n",
    "            models: List of fitted model objects.\n",
    "            x_data: Input features for prediction.\n",
    "            y_true: True target values.\n",
    "            model_names: List of model names corresponding to models.\n",
    "        \n",
    "        Returns:\n",
    "            Dict with model names as keys and metric results as values.\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        for model, name in zip(models, model_names):\n",
    "            try:\n",
    "                y_pred = model.predict(x_data)\n",
    "                model_results = {}\n",
    "                for metric in self.metrics:\n",
    "                    model_results[metric.name()] = metric.calculate(y_true, y_pred)\n",
    "                results[name] = model_results\n",
    "            except Exception as e:\n",
    "                print(f\"Error evaluating model {name}: {e}\")\n",
    "        return results\n",
    "\n",
    "    def save_results_to_csv(self, results, save_path):\n",
    "        \"\"\"Save evaluation results to a CSV file.\n",
    "        \n",
    "        Args:\n",
    "            results: Dict with model names as keys and metric results as values.\n",
    "            save_path: Path to save the CSV file.\n",
    "        \"\"\"\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        data = {\n",
    "            'Model': [],\n",
    "            'Metric': [],\n",
    "            'Value': []\n",
    "        }\n",
    "        for model_name, metrics in results.items():\n",
    "            for metric_name, value in metrics.items():\n",
    "                data['Model'].append(model_name)\n",
    "                data['Metric'].append(metric_name)\n",
    "                data['Value'].append(value)\n",
    "        df = pd.DataFrame(data)\n",
    "        df.to_csv(save_path, index=False)\n",
    "        print(f\"Evaluation results saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54b4731",
   "metadata": {},
   "source": [
    "UTILITY FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9673d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_actual_vs_predicted(y_true, y_pred, model_name, save_path):\n",
    "    \"\"\"Plot actual vs predicted values for a specific model.\"\"\"\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.scatterplot(x=y_true, y=y_pred)\n",
    "    plt.xlabel(\"Actual Yield (kg/ha)\")\n",
    "    plt.ylabel(\"Predicted Yield (kg/ha)\")\n",
    "    plt.title(f\"Actual vs Predicted Rice Yield ({model_name})\")\n",
    "    plt.grid(True)\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()\n",
    "\n",
    "def plot_residuals(y_true, y_pred, model_name, save_path):\n",
    "    \"\"\"Plot residuals distribution for a specific model.\"\"\"\n",
    "    residuals = np.array(y_true) - np.array(y_pred)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.histplot(residuals, kde=True)\n",
    "    plt.title(f\"Residuals Distribution ({model_name})\")\n",
    "    plt.xlabel(\"Prediction Error\")\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbd1c4e",
   "metadata": {},
   "source": [
    "UNIT TESTING TESTCASES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7d4dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestMetrics(unittest.TestCase):\n",
    "    \"\"\"Enhanced unit tests for metrics and evaluator.\"\"\"\n",
    "    \n",
    "    def setUp(self):\n",
    "        # Valid test data\n",
    "        self.y_true = [1, 2, 3, 4, 5]\n",
    "        self.y_pred = [1.1, 2.2, 2.8, 4.1, 4.9]\n",
    "        \n",
    "        # Create a temp directory and CSV\n",
    "        self.test_dir = tempfile.mkdtemp()\n",
    "        self.temp_csv = os.path.join(self.test_dir, \"temp_test.csv\")\n",
    "        df = pd.DataFrame({\"Actual\": self.y_true, \"Predicted\": self.y_pred})\n",
    "        df.to_csv(self.temp_csv, index=False)\n",
    "\n",
    "        # For evaluator\n",
    "        self.metrics = [R2Metric(), MAEMetric(), RMSEMetric(), MAPEMetric()]\n",
    "        self.evaluator = Evaluator(self.metrics)\n",
    "\n",
    "        # Mock models for testing\n",
    "        from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "        try:\n",
    "            from xgboost import XGBRegressor\n",
    "            self.xgb_available = True\n",
    "        except ImportError:\n",
    "            self.xgb_available = False\n",
    "        \n",
    "        self.models = [\n",
    "            RandomForestRegressor(n_estimators=10, random_state=42),\n",
    "            GradientBoostingRegressor(n_estimators=10, random_state=42)\n",
    "        ]\n",
    "        self.model_names = [\"RandomForest\", \"GradientBoosting\"]\n",
    "        if self.xgb_available:\n",
    "            self.models.append(XGBRegressor(n_estimators=10, random_state=42, verbosity=0))\n",
    "            self.model_names.append(\"XGBoost\")\n",
    "        \n",
    "        # Mock training data\n",
    "        self.x_train = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]])\n",
    "        self.y_train = np.array(self.y_true)\n",
    "        for model in self.models:\n",
    "            model.fit(self.x_train, self.y_train)\n",
    "    \n",
    "    def tearDown(self):\n",
    "        shutil.rmtree(self.test_dir)\n",
    "\n",
    "    @parameterized.expand([\n",
    "        (\"R2\", R2Metric(), r2_score),\n",
    "        (\"MAE\", MAEMetric(), mean_absolute_error),\n",
    "        (\"RMSE\", RMSEMetric(), lambda y, yp: np.sqrt(mean_squared_error(np.array(y), np.array(yp)))),\n",
    "        (\"sMAPE\", MAPEMetric(), lambda y, yp: np.mean(np.abs(np.array(y) - np.array(yp)) / (np.abs(np.array(y)) + np.abs(np.array(yp)) + 1e-10) * 100))\n",
    "    ])\n",
    "    \n",
    "    def test_metric_calculation(self, name, metric, reference_fn):\n",
    "        result = metric.calculate(self.y_true, self.y_pred)\n",
    "        expected = reference_fn(self.y_true, self.y_pred)\n",
    "        self.assertAlmostEqual(result, expected, places=6)\n",
    "\n",
    "    def test_metric_metadata(self):\n",
    "        for metric in self.metrics:\n",
    "            self.assertIsInstance(metric.name(), str)\n",
    "            self.assertTrue(len(metric.name()) > 0)\n",
    "            self.assertIsInstance(metric.description(), str)\n",
    "\n",
    "    def test_evaluator_from_csv(self):\n",
    "        results = self.evaluator.evaluate_from_csv(self.temp_csv)\n",
    "        self.assertIn(\"R2\", results)\n",
    "        self.assertIn(\"MAE\", results)\n",
    "        self.assertIn(\"RMSE\", results)\n",
    "        self.assertAlmostEqual(results[\"R2\"], r2_score(self.y_true, self.y_pred), places=6)\n",
    "\n",
    "    def test_evaluate_models(self):\n",
    "        results = self.evaluator.evaluate_models(self.models, self.x_train, self.y_train, self.model_names)\n",
    "        for model_name in self.model_names:\n",
    "            self.assertIn(model_name, results)\n",
    "            self.assertIn(\"R2\", results[model_name])\n",
    "            self.assertIn(\"MAE\", results[model_name])\n",
    "            self.assertIn(\"RMSE\", results[model_name])\n",
    "\n",
    "    def test_save_results_to_csv(self):\n",
    "        results = self.evaluator.evaluate_models(self.models, self.x_train, self.y_train, self.model_names)\n",
    "        save_path = os.path.join(self.test_dir, \"results.csv\")\n",
    "        self.evaluator.save_results_to_csv(results, save_path)\n",
    "        self.assertTrue(os.path.exists(save_path))\n",
    "        df = pd.read_csv(save_path)\n",
    "        self.assertEqual(len(df), len(self.metrics) * len(self.models))\n",
    "\n",
    "    def test_invalid_csv_file_not_found(self):\n",
    "        with self.assertRaises(FileNotFoundError):\n",
    "            self.evaluator.evaluate_from_csv(\"/nonexistent/path/file.csv\")\n",
    "    \n",
    "    def test_invalid_csv_missing_columns(self):\n",
    "        df = pd.DataFrame({\"Wrong\": self.y_true})\n",
    "        bad_csv = os.path.join(self.test_dir, \"bad.csv\")\n",
    "        df.to_csv(bad_csv, index=False)\n",
    "        with self.assertRaises(ValueError):\n",
    "            self.evaluator.evaluate_from_csv(bad_csv)\n",
    "    \n",
    "    def test_invalid_input_shapes(self):\n",
    "        metric = R2Metric()\n",
    "        with self.assertRaises(ValueError):\n",
    "            metric.calculate([1, 2], [1, 2, 3])\n",
    "    \n",
    "    def test_empty_inputs(self):\n",
    "        metric = R2Metric()\n",
    "        with self.assertRaises(ValueError):\n",
    "            metric.calculate([], [])\n",
    "\n",
    "    def test_non_numeric_inputs(self):\n",
    "        metric = R2Metric()\n",
    "        with self.assertRaises(ValueError):\n",
    "            metric.calculate([1, \"a\"], [1, 2])\n",
    "\n",
    "    def test_nan_inputs(self):\n",
    "        metric = R2Metric()\n",
    "        with self.assertRaises(ValueError):\n",
    "            metric.calculate([1, np.nan], [1, 2])\n",
    "\n",
    "    def test_inf_inputs(self):\n",
    "        metric = R2Metric()\n",
    "        with self.assertRaises(ValueError):\n",
    "            metric.calculate([1, 2], [1, np.inf])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65efa96f",
   "metadata": {},
   "source": [
    "MAIN EXECUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa06b12",
   "metadata": {},
   "source": [
    "LOADING DATA + SPLITTING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc355aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df = pd.read_csv('/content/rice_data_outlier_removed.csv')\n",
    "    x = df[['Year', 'RICE AREA (1000 ha)', 'RICE PRODUCTION (1000 tons)', 'State_en']]\n",
    "    y = df['RICE YIELD (Kg per ha)']\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "    print(\"Data loaded and split successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Dataset file '/content/rice_data_outlier_removed.csv' not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17499bd9",
   "metadata": {},
   "source": [
    "SCALING TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fb6748",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    scaler = joblib.load('Models/scaler.pkl')\n",
    "    x_train_scaled = scaler.fit_transform(x_train)\n",
    "    x_test_scaled = scaler.transform(x_test)\n",
    "    print(\"Scaler loaded and test data scaled successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Scaler file 'Models/scaler.pkl' not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error scaling data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8a76e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These will store models and their names which will then be passed to Evaluator for evaluation\n",
    "models = []\n",
    "model_names = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10d7f42",
   "metadata": {},
   "source": [
    "LOADING MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c3284b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    rf_model = joblib.load('Models/rf_model.pkl')\n",
    "    models.append(rf_model)\n",
    "    model_names.append(\"RandomForest\")\n",
    "    print(\"RandomForest model loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: RandomForest model file 'Models/rf_model.pkl' not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94df2055",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    xgb_model = joblib.load('Models/xgb_model.pkl')\n",
    "    models.append(xgb_model)\n",
    "    model_names.append(\"XGBoost\")\n",
    "    print(\"XGBoost model loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"XGBoost model not loaded: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81994bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a test model added to test the modular functionality\n",
    "from sklearn.linear_model import LinearRegression\n",
    "try:\n",
    "    lr_model = LinearRegression()\n",
    "    lr_model.fit(x_train_scaled, y_train)\n",
    "    models.append(lr_model)\n",
    "    model_names.append(\"LinearRegression\")\n",
    "    print(\"LinearRegression model trained and added successfully.\")\n",
    "except (FileNotFoundError, ImportError) as e:\n",
    "    print(f\"LinearRegression model not trained or imported: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebede45",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not models:\n",
    "    print(\"Warning: No models loaded successfully. Please check model files and dependencies.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f8366a",
   "metadata": {},
   "source": [
    "UNIT TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4d472e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running unit tests...\")\n",
    "unittest.main(argv=[''], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2498834e",
   "metadata": {},
   "source": [
    "EVALUATION OF MODELS THROUGH METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f19c59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    if models:\n",
    "        evaluator = Evaluator([R2Metric(), MAEMetric(), RMSEMetric(), MAPEMetric()])\n",
    "        results = evaluator.evaluate_models(models, x_test_scaled, y_test, model_names)\n",
    "        \n",
    "        # Print results\n",
    "        print(\"\\nEvaluation Results:\")\n",
    "        for model_name, metrics in results.items():\n",
    "            print(f\"\\n{model_name}:\")\n",
    "            for metric_name, value in metrics.items():\n",
    "                print(f\"  {metric_name}: {value:.4f}\")\n",
    "    else:\n",
    "        print(\"No models available for evaluation.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error evaluating models: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bde0e9",
   "metadata": {},
   "source": [
    "SAVE EVALUATION RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b326130",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    if models and 'results' in locals():\n",
    "        evaluator.save_results_to_csv(results, 'Models/evaluation_results.csv')\n",
    "    else:\n",
    "        print(\"No evaluation results to save. Ensure models are loaded and evaluated.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving evaluation results: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a529b624",
   "metadata": {},
   "source": [
    "PLOT GRAPHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeac6cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    if models:\n",
    "        plot_dir = \"Plots\"\n",
    "        os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "        for model, model_name in zip(models, model_names):\n",
    "            y_pred = model.predict(x_test_scaled)\n",
    "\n",
    "            plot_actual_vs_predicted(y_true=y_test, y_pred=y_pred, model_name=model_name, save_path=os.path.join(plot_dir, f\"actual_vs_predicted_{model_name}.png\"))\n",
    "\n",
    "            plot_residuals(y_true=y_test, y_pred=y_pred, model_name=model_name, save_path=os.path.join(plot_dir, f\"residuals_{model_name}.png\"))\n",
    "            print(f\"Plots generated for {model_name}.\")\n",
    "    else:\n",
    "        print(\"No models available for plotting.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error generating plots: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb4c813",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
